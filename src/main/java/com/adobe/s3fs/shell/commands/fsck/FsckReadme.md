This document covers the two main functionalities of the `fsck` command group: 
 - Verifying the state of the metastore/operation log and patching the two in case of inconsistencies.
 - Performing a full restore of the metastore from the operation log.


# Verifying the state of the filesystem

## Flow of commands

* Run `java -cp $HADOOP_CLASSPATH:/path/to/s3-filesystem.jar com.adobe.s3fs.shell.S3FsCli fsck verify --bucket some-bucket --output-path /path/on/local/hdfs --s3-partitions <s3-partitions> --us-s3-mt-mapper --s3-threads-per-partition <ths-per-partition>`
  - This will run a M/R job scanning both S3 and DynamoDB.
  - The `--bucket` indicates the bucket we are verifying.
  - The `--output-path` points to a location on local HDFS where the commands to amend the filesystem will be dumped.
  - For `--use-s3-mt-mapper` and `s3-threads-per-partition` see the "Tuning the S3 scanning component of fsck verify and full restore" section
  - The `--meta-partitions` indicates how many mappers will read from DynamoDb. This can be in the low 100s (default is 256) since the DynamoDB scan is reasonably efficient and does few calls to DynamoDB.
  - After the job finishes you can check the counters:
    - PARTIAL_RESTORE_META_SCAN : how many objects were read from meta
    - PARTIAL_RESTORE_S3_OPLOG_SCAN : how many log entries were read from s3
    - PARTIAL_RESTORE_S3_PHY_DATA_SCAN: how many physical files were scanned from s3
    - PARTIAL_RESTORE_OP_LOG_AND_META_OK: how many objects in oplog and meta were in sync
    - PARTIAL_RESTORE_OP_LOG_UPDATE: how many oplog entries will be amended to reflect the current state of meta
    - PARTIAL_RESTORE_INVALID_STATE_META_ONLY: how many meta objects did not have a corresponding oplog entry (should not happen)
    - PARTIAL_RESTORE_INVALID_STATE_META_AND_PHY_DATA: how many meta objects do not have an oplog entry but have an associated physical file(should not happen)
    - PARTIAL_RESTORE_INVALID_STATE_OPLOG_AND_META: similar to PARTIAL_RESTORE_INVALID_STATE_META_ONLY (should not happen)
    - PARTIAL_RESTORE_INVALID_STATE_OPLOG_AND_META_WITH_NO_PHY_DATA: objects(with physical data committed) having both oplog entry and meta and no physical file (should not happend) 
    - PARTIAL_RESTORE_INVALID_STATE_META_COMMITTED_AND_NO_PHY_DATA: similar to PARTIAL_RESTORE_INVALID_STATE_OPLOG_AND_META_WITH_NO_PHY_DATA (should not happen)
    - PARTIAL_RESTORE_VALID_STATE_META_UNCOMMITTED_AND_NO_PHY_DATA: objects in meta that do not physical data committed.
    - PARTIAL_RESTORE_VALID_STATE_META_COMMITTED_WITH_DELETE_INACTIVE_PHY: number of physical file that are left dangling after overwrite of a file
    - PARTIAL_RESTORE_META_MISSING_WITH_PHY_DATA: physical files that do not have meta entries (this can happen if the deletion of physical files fails and meta deletion succeeds)
    - PARTIAL_RESTORE_META_MISSING_WITH_OPLOG: oplog entries that do not have meta entries (this can happen if oplog deletion fails and meta deletion succeeds)
* Run `java -cp $HADOOP_CLASSPATH:/path/to/s3-filesystem.jar com.adobe.s3fs.shell.S3FsCli fsck commandLoader --base-dir /path/on/local/hdfs --bucket some-bucket`
  - This will load all the commands generated by the previous command
    

# Fsck full restore

## Flow of commands

* Before proceeding with full restoring the metastore, purge any leftover from `DynamoDB`:
  - `java -cp $HADOOP_CLASSPATH:/path/to/s3-filesystem.jar com.adobe.s3fs.shell.S3FsCli tools purgeMeta --path ks://some-bucket`:
    - `path` is used to determine the bucket and using the bucket name the `DynamoDB` table is determined by looking at config `fs.s3k.metastore.dynamo.table.some-bucket`

* Run `phase1` of `fsck fullRestore` (`phase1` restores file type objects in metastore):
  - `java -cp $HADOOP_CLASSPATH:/path/to/s3-filesystem.jar com.adobe.s3fs.shell.S3FsCli fsck fullRestore --bucket some-bucket --output-path /path/to/fullrestore --phase-1 --use-s3-mt-mapper --s3-threads-per-partition <ths-per-partition>`, where:
    - `--bucket` is S3 bucket where data & operation logs are stored.
    - `--output-path` is local HDFS location where output for current phase is stored (e.g.: for `phase1` location will be `/path/to/fullrestore/phase1`). Under each phase path there will be additional directories for each object type to be reconcile/recover (e.g.: `metastore` for `DynamoDB` object recovery, `s3` for `S3` recovery or deletion, etc.).
    - Table name of `DynamoDB` is given by same config key (e.g.: `fs.s3k.metastore.dynamo.table.some-bucket`).
    - `--partitions` is a optional argument used to specify the number of mappers spawned.
    - For `--use-3-mt-mapper` and `s3-threads-per-partition` see the "Tuning the S3 scanning component of fsck verify and full restore" section

* Run `commandLoader` to load & run the commands stored during `phase1`:
  - `java -cp $HADOOP_CLASSPATH:/path/to/s3-filesystem.jar com.adobe.s3fs.shell.S3FsCli fsck commandLoader --base-dir /path/to/fullrestore/phase1 --bucket some-bucket`, where:
    - `--base-dir` is HDFS location of `phase1` output.
    - `--bucket` is the S3 bucket used as suffix for configuration key used to identify `DynamoDB` table (e.g. the key identified by `fs.s3k.metastore.dynamo.table.<bucket>`).

* Run `phase2` of `fsck fullRestore` (`phase2` restores directories type objects in metastore):
  - `java -cp $HADOOP_CLASSPATH:/path/to/s3-filesystem.jar com.adobe.s3fs.shell.S3FsCli fsck fullRestore --path ks://some-bucket --output-path /path/to/fullrestore --phase-2`, where:
    - `--path` is the root logical path prefix used when comparing against the logical paths stored in metastore.
    - `--output-path` is local HDFS location where output for current phase is stored (e.g.: for `phase2` location will be `/path/to/fullRestore/phase2`).
    
* Run `commandLoader` to load & run the commands stored during `phase2`:
  - `java -cp $HADOOP_CLASSPATH:/path/to/s3-filesystem.jar com.adobe.s3fs.shell.S3FsCli fsck commandLoader --base-dir /path/to/fullrestore/phase2 --bucket some-bucket`.


# Tuning the S3 scanning component of fsck verify and full restore

The `--s3-partitions`, `--use-s3-mt-mapper` and `--s3-threads-per-partition` work in conjunction to provide performance tuning for the s3 scanning of the operation log.
  - `--s3-partitions` indicates how many mappers will read from S3. It's good to have this at least as large as the number of concurrent mappers. 
  - `--use-s3-mt-mapper` indicates that multithreaded mapper should be used.
  - `--s3-threads-per-partition` indicates how many threads a mapper will spawn.
  - The overall parallelism level is `nr_of_concurrent_mappers * s3_threads_per_mapper`. Finding the correct values for these parameters required a bit of trial and error.
  - In general, it's better to keep the `--s3-threads-per-partition` low (2-5) and scale up increasing the number of parallel mappers  
  - As an example: an S3 bucket(scaled to 256 partitions and containing 50 mil oplog entries and 50 mil physical files) can be scanned with a configuration of 5000 `--s3-partitions`, 3 `--s3-threads-per-partition` and 600 concurrent mappers in 30 min.

